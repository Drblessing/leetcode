{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "Import the necessary libraries, including requests and Beautiful Soup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (900276621.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    from %pip install bs4\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import requests\n",
    "from %pip install bs4\n",
    "bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Web Page\n",
    "\n",
    "Use the requests library to fetch the web page that you want to scrape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the web page you want to scrape\n",
    "url = \"http://example.com\"\n",
    "\n",
    "# Use the requests library to fetch the web page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status of the request\n",
    "# A status code of 200 means the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the web page.\")\n",
    "else:\n",
    "    print(\"Failed to fetch the web page. Status code: \", response.status_code)\n",
    "\n",
    "# You can access the content of the response with response.content\n",
    "page_content = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse HTML with Beautiful Soup\n",
    "\n",
    "Use Beautiful Soup to parse the HTML of the web page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create a Beautiful Soup object and specify the parser\n",
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "# Parse the HTML of the web page with Beautiful Soup\n",
    "# This will allow us to navigate and search through the HTML\n",
    "parsed_html = soup.prettify()\n",
    "\n",
    "# Print the parsed HTML to check the result\n",
    "print(parsed_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data\n",
    "\n",
    "Use Beautiful Soup's methods to extract the data you're interested in from the parsed HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the parsed HTML, we can use Beautiful Soup's methods to extract the data we're interested in.\n",
    "\n",
    "# Let's say we're interested in all the hyperlinks on the page.\n",
    "# Hyperlinks are defined by the 'a' tag and the actual link is stored in the 'href' attribute.\n",
    "\n",
    "# Use the 'find_all' method to find all instances of the 'a' tag\n",
    "a_tags = soup.find_all(\"a\")\n",
    "\n",
    "# Create an empty list to store the links\n",
    "links = []\n",
    "\n",
    "# Loop through the 'a' tags\n",
    "for tag in a_tags:\n",
    "    # Use the 'get' method to get the 'href' attribute and append it to the links list\n",
    "    links.append(tag.get(\"href\"))\n",
    "\n",
    "# Print the links\n",
    "for link in links:\n",
    "    print(link)\n",
    "\n",
    "# Similarly, you can extract other data you're interested in by finding the appropriate tags and attributes.\n",
    "# For example, if you're interested in the text inside all the 'p' tags, you can do:\n",
    "p_tags = soup.find_all(\"p\")\n",
    "for tag in p_tags:\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Format Data\n",
    "\n",
    "Clean and format the extracted data into a more usable format, such as a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas library is already imported, no need to import again\n",
    "\n",
    "# Clean the data\n",
    "# Remove None values from the links list\n",
    "links = [link for link in links if link is not None]\n",
    "\n",
    "# Format the data\n",
    "# Convert the list of links into a pandas DataFrame\n",
    "df_links = pd.DataFrame(links, columns=[\"Links\"])\n",
    "\n",
    "# Similarly, clean and format the text inside 'p' tags\n",
    "# Remove None values\n",
    "p_texts = [tag.text for tag in p_tags if tag.text is not None]\n",
    "\n",
    "# Convert the list of texts into a pandas DataFrame\n",
    "df_texts = pd.DataFrame(p_texts, columns=[\"Texts\"])\n",
    "\n",
    "# Now we have two DataFrames: df_links and df_texts\n",
    "# You can further process these DataFrames as per your requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
